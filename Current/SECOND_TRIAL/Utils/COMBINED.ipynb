{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/local/lib/python3.9/site-packages\")\n",
    "\n",
    "import nltk, pandas as pd, numpy as np\n",
    "import re\n",
    "from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\n",
    "from nltk.tree import ParentedTree\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "from nrclex import NRCLex\n",
    "\n",
    "#nltk.download('vader_lexicon') \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "import pickle\n",
    "from fuzzywuzzy import fuzz\n",
    "#nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from string import digits\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_parser = CoreNLPDependencyParser(url='http://0.0.0.0:9000')\n",
    "pos_tagger = CoreNLPParser(url='http://0.0.0.0:9000', tagtype='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence (input_sent):\n",
    "    # Parse sentence using Stanford CoreNLP Parser\n",
    "    pos_type = pos_tagger.tag(input_sent.split())\n",
    "    parse_tree, = ParentedTree.convert(list(pos_tagger.parse(input_sent.split()))[0])\n",
    "    dep_type, = ParentedTree.convert(dep_parser.parse(input_sent.split()))\n",
    "    return pos_type, parse_tree, dep_type\n",
    "\n",
    "def multi_liaison (input_sent, output=['tagging','parse_tree','type_dep','spo','relation']):\n",
    "    pos_type, parse_tree, dep_type = convert_sentence(input_sent)\n",
    "    pos_sent = ' '.join([x[0]+'/'+x[1] for x in pos_type])\n",
    "    # Extract subject, predicate and object\n",
    "    subject, adjective = get_subject(parse_tree)\n",
    "    predicate = get_predicate(parse_tree)\n",
    "    objects = get_object(parse_tree)\n",
    "    # Generate the relations between subjects and objects\n",
    "    relation = get_relationship(dep_type, subject, predicate, objects)\n",
    "    if 'tagging' in output:\n",
    "        print('---TAGGING---')\n",
    "        print(pos_sent)\n",
    "        print()\n",
    "    if 'parse_tree' in output:\n",
    "        print('---PARSE TREE---')\n",
    "        parse_tree.pretty_print()\n",
    "        print()\n",
    "    if 'type_dep' in output:\n",
    "#         print('---TYPED DEPENDENCIES---')\n",
    "        li = []\n",
    "        for x in dep_type.triples(): li.append(list(x))\n",
    "        return li\n",
    "#         print()\n",
    "    if 'spo' in output:\n",
    "        print('---MULTI-LIAISON OUTPUT---')\n",
    "        print('Subject: ',len(subject))\n",
    "        for x in subject: print(' '.join(x))\n",
    "        print('Predicate: ',len(predicate))\n",
    "        for x in predicate: print(' '.join(x))\n",
    "        print('Object: ',len(objects))\n",
    "        for x in objects: print(' '.join(x))\n",
    "        print()\n",
    "    if 'relation' in output:\n",
    "        print('---RELATIONSHIP---')\n",
    "        for x in relation: print(x)\n",
    "\n",
    "def get_subject (parse_tree):\n",
    "    # Extract the nouns and adjectives from NP_subtree which is before the first / main VP_subtree\n",
    "    subject, adjective = [],[]\n",
    "    for s in parse_tree:\n",
    "        if s.label() == 'NP':\n",
    "            for t in s.subtrees(lambda y: y.label() in ['NN','NNP','NNS','NNPS','PRP']):\n",
    "                # Avoid empty or repeated values\n",
    "                if t.pos()[0] not in subject:\n",
    "                    subject.append(t.pos()[0])\n",
    "            for t in s.subtrees(lambda y: y.label().startswith('JJ')):\n",
    "                if t.pos()[0] not in adjective:\n",
    "                    adjective.append(t.pos()[0])\n",
    "    return subject, adjective\n",
    "\n",
    "def get_predicate (parse_tree):\n",
    "    # Extract the verbs from the VP_subtree\n",
    "    predicate = []\n",
    "    for s in parse_tree.subtrees(lambda x: x.label() == 'VP'):\n",
    "        for t in s.subtrees(lambda y: y.label().startswith('VB')):\n",
    "            if t.pos()[0] not in predicate:\n",
    "                predicate.append(t.pos()[0]) \n",
    "    return predicate\n",
    "\n",
    "def get_object (parse_tree):\n",
    "    # Extract the nouns from VP_NP_subtree\n",
    "    objects, output = [],[]\n",
    "    for s in parse_tree.subtrees(lambda x: x.label() == 'VP'):\n",
    "        for t in s.subtrees(lambda y: y.label() == 'NP'):\n",
    "            for u in t.subtrees(lambda z: z.label() in ['NN','NNP','NNS','NNPS','PRP$']):\n",
    "                output = u.pos()[0]\n",
    "                if u.left_sibling() is not None and u.left_sibling().label().startswith('JJ'):\n",
    "                    output += u.left_sibling().pos()[0]\n",
    "                if output not in objects:\n",
    "                    objects.append(output)\n",
    "    return objects\n",
    "\n",
    "def get_relationship (dep_type, subject, predicate, objects):\n",
    "    # Generate relations based on the relationship dependencies obtained from parse_tree.triples()\n",
    "    subject = [x[0] for x in subject]\n",
    "    predicate = [x[0] for x in predicate]\n",
    "    objects = [x[0] for x in objects]     \n",
    "    d1, d2, r1, r2, relation, s1, s2, subjs = [],[],[],[],[],[],[],[]\n",
    "    w1, w2, output = '','',''\n",
    "    for head, rel, dep in dep_type.triples():\n",
    "        if rel in ['nsubj','acl:relcl','conj']:\n",
    "            s1, s2 = head[0], dep[0]\n",
    "            if s2 in subject and s1 in predicate:\n",
    "                w1, w2 = s2, s1\n",
    "            elif s2 in predicate and (s1 in subject or s1 in objects):\n",
    "                w1, w2 = s1, s2\n",
    "            elif s2 in subject and s1 in subject:\n",
    "                subjs = [s1, s2]\n",
    "            if w1 != '' and w2 != '':\n",
    "                r1 = [w1, w2]\n",
    "        if rel in ['dobj','prep','nmod','conj']:\n",
    "            d1, d2 = head[0], dep[0]\n",
    "            if d1 in objects and d2 in objects: \n",
    "                r2 = [d1,d2]\n",
    "            elif d2 in objects:\n",
    "                r2 = [d2]\n",
    "            elif d1 in objects:\n",
    "                r2 = [d1]\n",
    "        if len(r1) != 0 and len(r2) != 0 and (r2[0] not in r1 and r2[-1] not in r1):\n",
    "            if len(subjs) != 0:\n",
    "                for n in subjs:\n",
    "                    output = '-'.join([n] + r1[-1:] + r2)\n",
    "                    if output not in relation:\n",
    "                        relation.append(output)\n",
    "            else:\n",
    "                output = '-'.join(r1+r2)\n",
    "                if output not in relation:\n",
    "                    relation.append(output)  \n",
    "    rm = [x for x in relation for y in relation if x != y and re.match(x,y)]\n",
    "    for x in rm: \n",
    "        if x in relation:\n",
    "            relation.remove(x)    \n",
    "    return relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoreNLP Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('test', 'NN'), 'nsubj', ('This', 'DT')],\n",
       " [('test', 'NN'), 'cop', ('is', 'VBZ')],\n",
       " [('test', 'NN'), 'det', ('a', 'DT')],\n",
       " [('test', 'NN'), 'punct', ('.', '.')]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_liaison(\"This is a test.\", output='type_dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(narrative):\n",
    "    li = tokenize.sent_tokenize(narrative)\n",
    "    cleaned = []\n",
    "    \n",
    "    for sentence in li:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence)\n",
    "        sentence = sentence.replace(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|rt|\\d+', '')\n",
    "        sentence = sentence.replace(r'^\\s+|\\s+$', '') \n",
    "        sentence = sentence.replace(r'[^\\w]', ' ')\n",
    "        sentence = sentence.translate(str.maketrans('','',digits))\n",
    "        sentence = sentence.translate(str.maketrans('', '', punctuation))\n",
    "        senntence = re.sub(r'[^\\w]', ' ', sentence)\n",
    "        sentence = ' '.join([w for w in sentence.split() if w not in (stopwords)])\n",
    "        cleaned.append(sentence)\n",
    "        \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_paragraph(narrative, column_name):\n",
    "    df['paragraphs'] = df[column_name].fillna('')\n",
    "    df['paragraphs'] = df['paragraphs'].str.lower()\n",
    "    df['paragraphs'] = df['paragraphs'].str.replace(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|rt|\\d+', '')\n",
    "    df['paragraphs'] = df['paragraphs'].str.replace(r'^\\s+|\\s+$', '') \n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda x: ' '.join([w for w in x.split() if w not in (stopwords)]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infile = open(\"./lexicon\", \"rb\")\n",
    "# lexicon = pickle.load(infile)\n",
    "# infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon = list(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_behavior_breakdown(string):\n",
    "    li = multi_liaison(string, output=\"type_dep\")\n",
    "    behaviors = {}\n",
    "    for group in li:\n",
    "        if (group[0][1].startswith('JJ') or group[0][1].startswith('VB')) and group[0][0] not in behaviors:\n",
    "            behaviors[group[0][0]] = []\n",
    "        if group[2][1].startswith('RB') and group[0][0] in behaviors.keys():\n",
    "            behaviors[group[0][0]].append(group[2][0])\n",
    "    return behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: check if behaviors are in lexicon\n",
    "def check_behaviors(behaviors, lexicon, threshold):\n",
    "    \n",
    "#     dictionary = get_behavior_breakdown(narrative)\n",
    "    \n",
    "    for word, modifier in behaviors.items():\n",
    "        if \"not\" in modifier:\n",
    "            continue\n",
    "        else:\n",
    "            for behavior in lexicon:\n",
    "                if word in lexicon:\n",
    "                    return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(string):\n",
    "    result = sid.polarity_scores(string)\n",
    "    if (result['compound'] > 0):\n",
    "        return \"POS\"\n",
    "    elif (result['compound'] == 0):\n",
    "        return \"NEU\"\n",
    "    else:\n",
    "        return \"NEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_breakdown(string):\n",
    "    text_object = NRCLex(string)\n",
    "    frequencies = text_object.affect_frequencies\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(sentiment, behavior):\n",
    "    if (sentiment == \"NEG\" or behavior == True):\n",
    "        return \"UNWELL\"\n",
    "    else:\n",
    "        return \"WELL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_v2(row, narrative, sentences):\n",
    "   while True: \n",
    "        try:\n",
    "            print(row.name)\n",
    "            dictionary = {}\n",
    "            true = 0\n",
    "            false = 0\n",
    "            _hasBehavior = False\n",
    "\n",
    "            for sentence in sentences:\n",
    "                behaviors = {}\n",
    "                if (sentence):\n",
    "                    behaviors = get_behavior_breakdown(sentence)\n",
    "                hasBehavior = check_behaviors(behaviors, lexicon, 90)\n",
    "\n",
    "                if hasBehavior == True:\n",
    "                    true+=1\n",
    "                else:\n",
    "                    false+=1\n",
    "\n",
    "                dictionary.update(behaviors)\n",
    "\n",
    "            sentiments = get_sentiment_breakdown(narrative)\n",
    "            sentiment_val = get_sentiment(narrative)\n",
    "\n",
    "            if true > false:\n",
    "                _hasBehavior = True\n",
    "            else:\n",
    "                _hasBehavior = False\n",
    "\n",
    "            if (sentiment_val == \"NEG\" or _hasBehavior == True):\n",
    "                return pd.Series([\"U\", dictionary, sentiments])\n",
    "            else:\n",
    "                return pd.Series([\"W\", None, None])\n",
    "            break;\n",
    "        except:\n",
    "            print(\"Sentence:\", sentence)\n",
    "            print(\"Error:\", row.name)\n",
    "            return pd.Series([None, None, None])\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
